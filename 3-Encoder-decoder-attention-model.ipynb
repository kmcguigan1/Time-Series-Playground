{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.regularizers import L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"N_PREV\": 60,\n",
    "    \"N_FWD\": 30,\n",
    "    \n",
    "    \"ENCODER_SIZE\": 16,\n",
    "    \"DECODER_SIZE\": 16,\n",
    "    \"ATTENTION_SIZE\": 8,\n",
    "    \n",
    "    \"ATTENTION_MODULE\": \"additive\",\n",
    "    \"ATTENTION_METHOD\": \"standard\",\n",
    "    \n",
    "    \"LR\": 0.001,\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"EPOCHS\": 30,\n",
    "}\n",
    "\n",
    "MODEL = f\"{config['ATTENTION_METHOD']}-{config['ATTENTION_MODULE']}-attention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkmcguigan\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\kiern/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from secrets import WANDB\n",
    "wandb.login(key=WANDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load Data Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(fold):\n",
    "    with open(f'./data/fold{fold}_data_v2.npy', mode='rb') as f:\n",
    "        X_train = np.load(f)\n",
    "        y_train = np.load(f)\n",
    "        X_val = np.load(f)\n",
    "        y_val = np.load(f)\n",
    "    # with open(f'./data/fold{fold}_normalizer_v2.pkl', mode='rb') as f:\n",
    "        # normalizer = pickle.load(f)\n",
    "    return X_train, y_train, X_val, y_val, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 60, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 60, 16),     1152        ['input_1[0][0]']                \n",
      "                                 (None, 16),                                                      \n",
      "                                 (None, 16)]                                                      \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 1)           0           ['input_1[0][0]']                \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.expand_dims_1 (TFOpLambda)  (None, 1, 16)        0           ['lstm[0][1]']                   \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, 1, 1)         0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " bahdanau_attention_layer (Bahd  (None, 1, 16)       272         ['tf.expand_dims_1[0][0]',       \n",
      " anauAttentionLayer)                                              'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_2[0][0]',       \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_3[0][0]',       \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_4[0][0]',       \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_5[0][0]',       \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_6[0][0]',       \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_7[0][0]',       \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_8[0][0]',       \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_9[0][0]',       \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_10[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_11[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_12[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_13[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_14[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_15[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_16[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_17[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_18[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_19[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_20[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_21[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_22[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_23[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_24[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_25[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_26[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_27[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_28[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_29[0][0]',      \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'tf.expand_dims_30[0][0]',      \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 1, 17)        0           ['tf.expand_dims[0][0]',         \n",
      "                                                                  'bahdanau_attention_layer[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 1, 16),      2176        ['tf.concat[0][0]',              \n",
      "                                 (None, 16),                      'lstm[0][1]',                   \n",
      "                                 (None, 16)]                      'lstm[0][2]',                   \n",
      "                                                                  'tf.concat_1[0][0]',            \n",
      "                                                                  'lstm_1[0][1]',                 \n",
      "                                                                  'lstm_1[0][2]',                 \n",
      "                                                                  'tf.concat_2[0][0]',            \n",
      "                                                                  'lstm_1[1][1]',                 \n",
      "                                                                  'lstm_1[1][2]',                 \n",
      "                                                                  'tf.concat_3[0][0]',            \n",
      "                                                                  'lstm_1[2][1]',                 \n",
      "                                                                  'lstm_1[2][2]',                 \n",
      "                                                                  'tf.concat_4[0][0]',            \n",
      "                                                                  'lstm_1[3][1]',                 \n",
      "                                                                  'lstm_1[3][2]',                 \n",
      "                                                                  'tf.concat_5[0][0]',            \n",
      "                                                                  'lstm_1[4][1]',                 \n",
      "                                                                  'lstm_1[4][2]',                 \n",
      "                                                                  'tf.concat_6[0][0]',            \n",
      "                                                                  'lstm_1[5][1]',                 \n",
      "                                                                  'lstm_1[5][2]',                 \n",
      "                                                                  'tf.concat_7[0][0]',            \n",
      "                                                                  'lstm_1[6][1]',                 \n",
      "                                                                  'lstm_1[6][2]',                 \n",
      "                                                                  'tf.concat_8[0][0]',            \n",
      "                                                                  'lstm_1[7][1]',                 \n",
      "                                                                  'lstm_1[7][2]',                 \n",
      "                                                                  'tf.concat_9[0][0]',            \n",
      "                                                                  'lstm_1[8][1]',                 \n",
      "                                                                  'lstm_1[8][2]',                 \n",
      "                                                                  'tf.concat_10[0][0]',           \n",
      "                                                                  'lstm_1[9][1]',                 \n",
      "                                                                  'lstm_1[9][2]',                 \n",
      "                                                                  'tf.concat_11[0][0]',           \n",
      "                                                                  'lstm_1[10][1]',                \n",
      "                                                                  'lstm_1[10][2]',                \n",
      "                                                                  'tf.concat_12[0][0]',           \n",
      "                                                                  'lstm_1[11][1]',                \n",
      "                                                                  'lstm_1[11][2]',                \n",
      "                                                                  'tf.concat_13[0][0]',           \n",
      "                                                                  'lstm_1[12][1]',                \n",
      "                                                                  'lstm_1[12][2]',                \n",
      "                                                                  'tf.concat_14[0][0]',           \n",
      "                                                                  'lstm_1[13][1]',                \n",
      "                                                                  'lstm_1[13][2]',                \n",
      "                                                                  'tf.concat_15[0][0]',           \n",
      "                                                                  'lstm_1[14][1]',                \n",
      "                                                                  'lstm_1[14][2]',                \n",
      "                                                                  'tf.concat_16[0][0]',           \n",
      "                                                                  'lstm_1[15][1]',                \n",
      "                                                                  'lstm_1[15][2]',                \n",
      "                                                                  'tf.concat_17[0][0]',           \n",
      "                                                                  'lstm_1[16][1]',                \n",
      "                                                                  'lstm_1[16][2]',                \n",
      "                                                                  'tf.concat_18[0][0]',           \n",
      "                                                                  'lstm_1[17][1]',                \n",
      "                                                                  'lstm_1[17][2]',                \n",
      "                                                                  'tf.concat_19[0][0]',           \n",
      "                                                                  'lstm_1[18][1]',                \n",
      "                                                                  'lstm_1[18][2]',                \n",
      "                                                                  'tf.concat_20[0][0]',           \n",
      "                                                                  'lstm_1[19][1]',                \n",
      "                                                                  'lstm_1[19][2]',                \n",
      "                                                                  'tf.concat_21[0][0]',           \n",
      "                                                                  'lstm_1[20][1]',                \n",
      "                                                                  'lstm_1[20][2]',                \n",
      "                                                                  'tf.concat_22[0][0]',           \n",
      "                                                                  'lstm_1[21][1]',                \n",
      "                                                                  'lstm_1[21][2]',                \n",
      "                                                                  'tf.concat_23[0][0]',           \n",
      "                                                                  'lstm_1[22][1]',                \n",
      "                                                                  'lstm_1[22][2]',                \n",
      "                                                                  'tf.concat_24[0][0]',           \n",
      "                                                                  'lstm_1[23][1]',                \n",
      "                                                                  'lstm_1[23][2]',                \n",
      "                                                                  'tf.concat_25[0][0]',           \n",
      "                                                                  'lstm_1[24][1]',                \n",
      "                                                                  'lstm_1[24][2]',                \n",
      "                                                                  'tf.concat_26[0][0]',           \n",
      "                                                                  'lstm_1[25][1]',                \n",
      "                                                                  'lstm_1[25][2]',                \n",
      "                                                                  'tf.concat_27[0][0]',           \n",
      "                                                                  'lstm_1[26][1]',                \n",
      "                                                                  'lstm_1[26][2]',                \n",
      "                                                                  'tf.concat_28[0][0]',           \n",
      "                                                                  'lstm_1[27][1]',                \n",
      "                                                                  'lstm_1[27][2]',                \n",
      "                                                                  'tf.concat_29[0][0]',           \n",
      "                                                                  'lstm_1[28][1]',                \n",
      "                                                                  'lstm_1[28][2]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1, 16)        0           ['lstm_1[0][0]',                 \n",
      "                                                                  'lstm_1[1][0]',                 \n",
      "                                                                  'lstm_1[2][0]',                 \n",
      "                                                                  'lstm_1[3][0]',                 \n",
      "                                                                  'lstm_1[4][0]',                 \n",
      "                                                                  'lstm_1[5][0]',                 \n",
      "                                                                  'lstm_1[6][0]',                 \n",
      "                                                                  'lstm_1[7][0]',                 \n",
      "                                                                  'lstm_1[8][0]',                 \n",
      "                                                                  'lstm_1[9][0]',                 \n",
      "                                                                  'lstm_1[10][0]',                \n",
      "                                                                  'lstm_1[11][0]',                \n",
      "                                                                  'lstm_1[12][0]',                \n",
      "                                                                  'lstm_1[13][0]',                \n",
      "                                                                  'lstm_1[14][0]',                \n",
      "                                                                  'lstm_1[15][0]',                \n",
      "                                                                  'lstm_1[16][0]',                \n",
      "                                                                  'lstm_1[17][0]',                \n",
      "                                                                  'lstm_1[18][0]',                \n",
      "                                                                  'lstm_1[19][0]',                \n",
      "                                                                  'lstm_1[20][0]',                \n",
      "                                                                  'lstm_1[21][0]',                \n",
      "                                                                  'lstm_1[22][0]',                \n",
      "                                                                  'lstm_1[23][0]',                \n",
      "                                                                  'lstm_1[24][0]',                \n",
      "                                                                  'lstm_1[25][0]',                \n",
      "                                                                  'lstm_1[26][0]',                \n",
      "                                                                  'lstm_1[27][0]',                \n",
      "                                                                  'lstm_1[28][0]',                \n",
      "                                                                  'lstm_1[29][0]']                \n",
      "                                                                                                  \n",
      " tf.expand_dims_2 (TFOpLambda)  (None, 1, 16)        0           ['lstm_1[0][1]']                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1, 1)         17          ['dropout[0][0]',                \n",
      "                                                                  'dropout[1][0]',                \n",
      "                                                                  'dropout[2][0]',                \n",
      "                                                                  'dropout[3][0]',                \n",
      "                                                                  'dropout[4][0]',                \n",
      "                                                                  'dropout[5][0]',                \n",
      "                                                                  'dropout[6][0]',                \n",
      "                                                                  'dropout[7][0]',                \n",
      "                                                                  'dropout[8][0]',                \n",
      "                                                                  'dropout[9][0]',                \n",
      "                                                                  'dropout[10][0]',               \n",
      "                                                                  'dropout[11][0]',               \n",
      "                                                                  'dropout[12][0]',               \n",
      "                                                                  'dropout[13][0]',               \n",
      "                                                                  'dropout[14][0]',               \n",
      "                                                                  'dropout[15][0]',               \n",
      "                                                                  'dropout[16][0]',               \n",
      "                                                                  'dropout[17][0]',               \n",
      "                                                                  'dropout[18][0]',               \n",
      "                                                                  'dropout[19][0]',               \n",
      "                                                                  'dropout[20][0]',               \n",
      "                                                                  'dropout[21][0]',               \n",
      "                                                                  'dropout[22][0]',               \n",
      "                                                                  'dropout[23][0]',               \n",
      "                                                                  'dropout[24][0]',               \n",
      "                                                                  'dropout[25][0]',               \n",
      "                                                                  'dropout[26][0]',               \n",
      "                                                                  'dropout[27][0]',               \n",
      "                                                                  'dropout[28][0]',               \n",
      "                                                                  'dropout[29][0]']               \n",
      "                                                                                                  \n",
      " tf.concat_1 (TFOpLambda)       (None, 1, 17)        0           ['dense_2[0][0]',                \n",
      "                                                                  'bahdanau_attention_layer[1][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_3 (TFOpLambda)  (None, 1, 16)        0           ['lstm_1[1][1]']                 \n",
      "                                                                                                  \n",
      " tf.concat_2 (TFOpLambda)       (None, 1, 17)        0           ['dense_2[1][0]',                \n",
      "                                                                  'bahdanau_attention_layer[2][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_4 (TFOpLambda)  (None, 1, 16)        0           ['lstm_1[2][1]']                 \n",
      "                                                                                                  \n",
      " tf.concat_3 (TFOpLambda)       (None, 1, 17)        0           ['dense_2[2][0]',                \n",
      "                                                                  'bahdanau_attention_layer[3][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_5 (TFOpLambda)  (None, 1, 16)        0           ['lstm_1[3][1]']                 \n",
      "                                                                                                  \n",
      " tf.concat_4 (TFOpLambda)       (None, 1, 17)        0           ['dense_2[3][0]',                \n",
      "                                                                  'bahdanau_attention_layer[4][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_6 (TFOpLambda)  (None, 1, 16)        0           ['lstm_1[4][1]']                 \n",
      "                                                                                                  \n",
      " tf.concat_5 (TFOpLambda)       (None, 1, 17)        0           ['dense_2[4][0]',                \n",
      "                                                                  'bahdanau_attention_layer[5][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_7 (TFOpLambda)  (None, 1, 16)        0           ['lstm_1[5][1]']                 \n",
      "                                                                                                  \n",
      " tf.concat_6 (TFOpLambda)       (None, 1, 17)        0           ['dense_2[5][0]',                \n",
      "                                                                  'bahdanau_attention_layer[6][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_8 (TFOpLambda)  (None, 1, 16)        0           ['lstm_1[6][1]']                 \n",
      "                                                                                                  \n",
      " tf.concat_7 (TFOpLambda)       (None, 1, 17)        0           ['dense_2[6][0]',                \n",
      "                                                                  'bahdanau_attention_layer[7][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_9 (TFOpLambda)  (None, 1, 16)        0           ['lstm_1[7][1]']                 \n",
      "                                                                                                  \n",
      " tf.concat_8 (TFOpLambda)       (None, 1, 17)        0           ['dense_2[7][0]',                \n",
      "                                                                  'bahdanau_attention_layer[8][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_10 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[8][1]']                 \n",
      "                                                                                                  \n",
      " tf.concat_9 (TFOpLambda)       (None, 1, 17)        0           ['dense_2[8][0]',                \n",
      "                                                                  'bahdanau_attention_layer[9][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_11 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[9][1]']                 \n",
      "                                                                                                  \n",
      " tf.concat_10 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[9][0]',                \n",
      "                                                                  'bahdanau_attention_layer[10][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_12 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[10][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_11 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[10][0]',               \n",
      "                                                                  'bahdanau_attention_layer[11][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_13 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[11][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_12 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[11][0]',               \n",
      "                                                                  'bahdanau_attention_layer[12][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_14 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[12][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_13 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[12][0]',               \n",
      "                                                                  'bahdanau_attention_layer[13][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_15 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[13][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_14 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[13][0]',               \n",
      "                                                                  'bahdanau_attention_layer[14][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_16 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[14][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_15 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[14][0]',               \n",
      "                                                                  'bahdanau_attention_layer[15][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_17 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[15][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_16 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[15][0]',               \n",
      "                                                                  'bahdanau_attention_layer[16][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_18 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[16][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_17 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[16][0]',               \n",
      "                                                                  'bahdanau_attention_layer[17][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_19 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[17][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_18 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[17][0]',               \n",
      "                                                                  'bahdanau_attention_layer[18][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_20 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[18][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_19 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[18][0]',               \n",
      "                                                                  'bahdanau_attention_layer[19][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_21 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[19][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_20 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[19][0]',               \n",
      "                                                                  'bahdanau_attention_layer[20][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_22 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[20][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_21 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[20][0]',               \n",
      "                                                                  'bahdanau_attention_layer[21][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_23 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[21][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_22 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[21][0]',               \n",
      "                                                                  'bahdanau_attention_layer[22][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_24 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[22][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_23 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[22][0]',               \n",
      "                                                                  'bahdanau_attention_layer[23][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_25 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[23][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_24 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[23][0]',               \n",
      "                                                                  'bahdanau_attention_layer[24][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_26 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[24][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_25 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[24][0]',               \n",
      "                                                                  'bahdanau_attention_layer[25][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_27 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[25][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_26 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[25][0]',               \n",
      "                                                                  'bahdanau_attention_layer[26][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_28 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[26][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_27 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[26][0]',               \n",
      "                                                                  'bahdanau_attention_layer[27][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_29 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[27][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_28 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[27][0]',               \n",
      "                                                                  'bahdanau_attention_layer[28][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_30 (TFOpLambda)  (None, 1, 16)       0           ['lstm_1[28][1]']                \n",
      "                                                                                                  \n",
      " tf.concat_29 (TFOpLambda)      (None, 1, 17)        0           ['dense_2[28][0]',               \n",
      "                                                                  'bahdanau_attention_layer[29][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 30, 1)        0           ['dense_2[0][0]',                \n",
      "                                                                  'dense_2[1][0]',                \n",
      "                                                                  'dense_2[2][0]',                \n",
      "                                                                  'dense_2[3][0]',                \n",
      "                                                                  'dense_2[4][0]',                \n",
      "                                                                  'dense_2[5][0]',                \n",
      "                                                                  'dense_2[6][0]',                \n",
      "                                                                  'dense_2[7][0]',                \n",
      "                                                                  'dense_2[8][0]',                \n",
      "                                                                  'dense_2[9][0]',                \n",
      "                                                                  'dense_2[10][0]',               \n",
      "                                                                  'dense_2[11][0]',               \n",
      "                                                                  'dense_2[12][0]',               \n",
      "                                                                  'dense_2[13][0]',               \n",
      "                                                                  'dense_2[14][0]',               \n",
      "                                                                  'dense_2[15][0]',               \n",
      "                                                                  'dense_2[16][0]',               \n",
      "                                                                  'dense_2[17][0]',               \n",
      "                                                                  'dense_2[18][0]',               \n",
      "                                                                  'dense_2[19][0]',               \n",
      "                                                                  'dense_2[20][0]',               \n",
      "                                                                  'dense_2[21][0]',               \n",
      "                                                                  'dense_2[22][0]',               \n",
      "                                                                  'dense_2[23][0]',               \n",
      "                                                                  'dense_2[24][0]',               \n",
      "                                                                  'dense_2[25][0]',               \n",
      "                                                                  'dense_2[26][0]',               \n",
      "                                                                  'dense_2[27][0]',               \n",
      "                                                                  'dense_2[28][0]',               \n",
      "                                                                  'dense_2[29][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,617\n",
      "Trainable params: 3,617\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class BahdanauAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, use_bias=False):\n",
    "        super(BahdanauAttentionLayer, self).__init__()\n",
    "        self.units = units\n",
    "        self.use_bias = use_bias\n",
    "        self.W1 = tf.keras.layers.Dense(self.units, use_bias=self.use_bias)\n",
    "        self.W2 = tf.keras.layers.Dense(self.units, use_bias=self.use_bias)\n",
    "    def get_config(self):\n",
    "        config = super(BahdanauAttentionLayer, self).get_config()\n",
    "        config.update({\"units\": self.units, \"use_bias\":self.use_bias})\n",
    "        return config\n",
    "    def call(self, query, values, keys=None, verbose=False):\n",
    "        expanded_query = tf.expand_dims(query, 2)\n",
    "        encoded_query = self.W1(expanded_query)\n",
    "        if(keys is None):\n",
    "            encoded_keys = self.W2(tf.expand_dims(values, 1))\n",
    "        else:\n",
    "            encoded_keys = self.W2(tf.expand_dims(keys, 1))\n",
    "        combined_encoded_query_and_keys = encoded_query + encoded_keys\n",
    "        tanh_score = tf.nn.tanh(combined_encoded_query_and_keys)\n",
    "        score = tf.reduce_sum(tanh_score, axis=-1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=-1)\n",
    "        overall_context_vector = tf.matmul(attention_weights, values)\n",
    "        context_vector = overall_context_vector\n",
    "        return context_vector\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, use_bias=False):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.units = units\n",
    "        self.use_bias = use_bias\n",
    "        self.W1 = tf.keras.layers.Dense(self.units, use_bias=self.use_bias)\n",
    "        self.W2 = tf.keras.layers.Dense(self.units, use_bias=self.use_bias)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    def get_config(self):\n",
    "        config = super(BahdanauAttention, self).get_config()\n",
    "        config.update({\"units\": self.units, \"use_bias\":self.use_bias})\n",
    "        return config\n",
    "    def call(self, query, values, keys=None, verbose=False):\n",
    "        query_with_time_axis = tf.expand_dims(query, 2)\n",
    "        encoded_query = self.W1(query_with_time_axis)\n",
    "        if(keys is None):\n",
    "            encoded_keys = self.W2(tf.expand_dims(values, 1))\n",
    "        else:\n",
    "            encoded_keys = self.W2(tf.expand_dims(keys, 1))\n",
    "        combined_encoded_query_and_keys = encoded_query + encoded_keys\n",
    "        tanh_score = tf.nn.tanh(combined_encoded_query_and_keys)\n",
    "        score = self.V(tanh_score)\n",
    "        squeezed_score = tf.squeeze(score, axis=-1)\n",
    "        attention_weights = tf.nn.softmax(squeezed_score, axis=-1)\n",
    "        context_vector = tf.matmul(attention_weights, values)\n",
    "        return context_vector\n",
    "    \n",
    "class LuongAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, use_bias=False):\n",
    "        super(BahdanauAttentionLayer, self).__init__()\n",
    "        self.units = units\n",
    "        self.use_bias = use_bias\n",
    "        self.W1 = tf.keras.layers.Dense(self.units, use_bias=self.use_bias)\n",
    "        self.W2 = tf.keras.layers.Dense(self.units, use_bias=self.use_bias)\n",
    "    def get_config(self):\n",
    "        config = super(BahdanauAttentionLayer, self).get_config()\n",
    "        config.update({\"units\": self.units, \"use_bias\":self.use_bias})\n",
    "        return config\n",
    "    def call(self, query, values, keys=None, verbose=False):\n",
    "        expanded_query = tf.expand_dims(query, 2)\n",
    "        encoded_query = self.W1(expanded_query)\n",
    "        if(keys is None):\n",
    "            encoded_keys = self.W2(tf.expand_dims(values, 1))\n",
    "        else:\n",
    "            keys = tf.expand_dims(keys, 1)\n",
    "            encoded_keys = self.W2(keys)\n",
    "        combined_encoded_query_and_keys = encoded_query * encoded_keys\n",
    "        tanh_score = tf.nn.tanh(combined_encoded_query_and_keys)\n",
    "        score = tf.reduce_sum(tanh_score, axis=-1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=-1)\n",
    "        overall_context_vector = tf.matmul(attention_weights, values)\n",
    "        context_vector = overall_context_vector\n",
    "        return context_vector\n",
    "\n",
    "class LuongAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, use_bias=False):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.units = units\n",
    "        self.use_bias = use_bias\n",
    "        self.W1 = tf.keras.layers.Dense(self.units, use_bias=self.use_bias)\n",
    "        self.W2 = tf.keras.layers.Dense(self.units, use_bias=self.use_bias)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    def get_config(self):\n",
    "        config = super(BahdanauAttention, self).get_config()\n",
    "        config.update({\"units\": self.units, \"use_bias\":self.use_bias})\n",
    "        return config\n",
    "    def call(self, query, values, keys=None, verbose=False):\n",
    "        query_with_time_axis = tf.expand_dims(query, 2)\n",
    "        encoded_query = self.W1(query_with_time_axis)\n",
    "        if(keys is None):\n",
    "            encoded_keys = self.W2(tf.expand_dims(values, 1))\n",
    "        else:\n",
    "            keys = tf.expand_dims(keys, 1)\n",
    "            encoded_keys = self.W2(keys)\n",
    "        combined_encoded_query_and_keys = encoded_query * encoded_keys\n",
    "        tanh_score = tf.nn.tanh(combined_encoded_query_and_keys)\n",
    "        score = self.V(tanh_score)\n",
    "        squeezed_score = tf.squeeze(score, axis=-1)\n",
    "        attention_weights = tf.nn.softmax(squeezed_score, axis=-1)\n",
    "        context_vector = tf.matmul(attention_weights, values)\n",
    "        return context_vector\n",
    "\n",
    "def simple_encoder_decoder():\n",
    "    inputs = tf.keras.layers.Input(shape=(config[\"N_PREV\"], 1))\n",
    "    encoder_outputs, hidden_state, cell_state = tf.keras.layers.LSTM(config[\"ENCODER_SIZE\"], \n",
    "                                                       return_sequences=True, \n",
    "                                                       return_state=True, \n",
    "                                                       recurrent_initializer='glorot_uniform', \n",
    "                                                       activity_regularizer=L1L2(l1=0.000001, l2=0.000001))(inputs)\n",
    "    \n",
    "    if(config[\"ATTENTION_MODULE\"] == \"additive\"):\n",
    "        if(config[\"ATTENTION_METHOD\"] == \"standard\"):\n",
    "            attention_layer = BahdanauAttentionLayer(config[\"ATTENTION_SIZE\"], use_bias=True)\n",
    "        elif(config[\"ATTENTION_METHOD\"] == \"vectorized\"):\n",
    "            attention_layer = BahdanauAttention(config[\"ATTENTION_SIZE\"], use_bias=True)\n",
    "        else:\n",
    "            raise Exception(f\"Invalid config ATTENTION_METHOD of {config['ATTENTION_METHOD']}\")\n",
    "    elif(config[\"ATTENTION_MODULE\"] == \"multiplicative\"):\n",
    "        if(config[\"ATTENTION_METHOD\"] == \"standard\"):\n",
    "            attention_layer = LuongAttentionLayer(config[\"ATTENTION_SIZE\"], use_bias=True)\n",
    "        elif(config[\"ATTENTION_METHOD\"] == \"vectorized\"):\n",
    "            attention_layer = LuongAttention(config[\"ATTENTION_SIZE\"], use_bias=True)\n",
    "        else:\n",
    "            raise Exception(f\"Invalid config ATTENTION_METHOD of {config['ATTENTION_METHOD']}\")\n",
    "    else:\n",
    "        raise Exception(f\"Invalid config ATTENTION_MODULE of {config['ATTENTION_MODULE']}\")\n",
    "    decoder = tf.keras.layers.LSTM(config[\"DECODER_SIZE\"],\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform', \n",
    "                                   activity_regularizer=L1L2(l1=0.000001, l2=0.000001))\n",
    "    dropout = tf.keras.layers.Dropout(0.2)\n",
    "    decoder_output = tf.keras.layers.Dense(1)\n",
    "    all_outputs = []\n",
    "    last_value = tf.expand_dims(inputs[:, -1, 0:1], 1)\n",
    "    states = [hidden_state, cell_state]\n",
    "    for i in range(config[\"N_FWD\"]):\n",
    "        use_verbose = False\n",
    "        if(i == 0):\n",
    "            use_verbose=True\n",
    "        context_vector = attention_layer(query=tf.expand_dims(states[0],1), values=encoder_outputs, verbose=use_verbose)\n",
    "        decoder_input = tf.concat((last_value, context_vector), axis=-1)\n",
    "        x, hidden_state, cell_state = decoder(decoder_input, initial_state=states)\n",
    "        states=[hidden_state, cell_state]\n",
    "        x = dropout(x)\n",
    "        last_value = decoder_output(x)\n",
    "        all_outputs.append(last_value)\n",
    "    outputs = tf.keras.layers.Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss=\"mse\", metrics=[\"mae\"], optimizer=tf.keras.optimizers.Adam(learning_rate=config[\"LR\"]))\n",
    "    return model\n",
    "\n",
    "model = simple_encoder_decoder()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126967"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val):\n",
    "    reducer = tf.keras.callbacks.ReduceLROnPlateau(monior='val_loss', factor=0.1, patience=2, mode='min', cooldown=1)\n",
    "    stopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, mode='min', restore_best_weights=True)\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=config[\"BATCH_SIZE\"],\n",
    "              epochs=config[\"EPOCHS\"], \n",
    "              callbacks=[reducer, stopper, WandbCallback()],\n",
    "              validation_data=(X_val, y_val),\n",
    "              validation_batch_size=config[\"BATCH_SIZE\"],\n",
    "              shuffle=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for fold in range(1, 4):\n",
    "        run = wandb.init(project=\"time-series-methods\", entity=\"kmcguigan\", group=f\"{MODEL}-model\", config=config, job_type=\"train\")\n",
    "        run.name = f'{MODEL}-fold-{fold}'\n",
    "        X_train, y_train, X_val, y_val, _ = get_data(fold)\n",
    "        model = simple_encoder_decoder()\n",
    "        model = train_model(model, X_train, y_train, X_val, y_val)\n",
    "        run.finish()\n",
    "        del model\n",
    "        del X_train\n",
    "        del y_train\n",
    "        del X_val\n",
    "        del y_val\n",
    "        gc.collect()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\kiern\\MyFolders\\Code\\GitRepositories\\Time-Series-Playground\\wandb\\run-20220402_191130-o7660oed</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kmcguigan/time-series-methods/runs/o7660oed\" target=\"_blank\">zany-butterfly-14</a></strong> to <a href=\"https://wandb.ai/kmcguigan/time-series-methods\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2485/2485 [==============================] - 248s 76ms/step - loss: 0.3175 - mae: 0.4272 - val_loss: 0.1832 - val_mae: 0.3231 - lr: 0.0010 - _timestamp: 1648948552.0000 - _runtime: 262.0000\n",
      "Epoch 2/30\n",
      "2485/2485 [==============================] - 169s 68ms/step - loss: 0.1898 - mae: 0.3282 - val_loss: 0.1457 - val_mae: 0.2830 - lr: 0.0010 - _timestamp: 1648948721.0000 - _runtime: 431.0000\n",
      "Epoch 3/30\n",
      "2485/2485 [==============================] - 176s 71ms/step - loss: 0.1708 - mae: 0.3094 - val_loss: 0.1356 - val_mae: 0.2723 - lr: 0.0010 - _timestamp: 1648948896.0000 - _runtime: 606.0000\n",
      "Epoch 4/30\n",
      "2485/2485 [==============================] - 138s 55ms/step - loss: 0.1670 - mae: 0.3052 - val_loss: 0.1388 - val_mae: 0.2715 - lr: 0.0010 - _timestamp: 1648949034.0000 - _runtime: 744.0000\n",
      "Epoch 5/30\n",
      "2485/2485 [==============================] - 145s 58ms/step - loss: 0.1692 - mae: 0.3058 - val_loss: 0.1338 - val_mae: 0.2703 - lr: 0.0010 - _timestamp: 1648949179.0000 - _runtime: 889.0000\n",
      "Epoch 6/30\n",
      "2485/2485 [==============================] - 157s 63ms/step - loss: 0.1746 - mae: 0.3114 - val_loss: 0.1520 - val_mae: 0.2898 - lr: 0.0010 - _timestamp: 1648949336.0000 - _runtime: 1046.0000\n",
      "Epoch 7/30\n",
      "2485/2485 [==============================] - 160s 64ms/step - loss: 0.1602 - mae: 0.2976 - val_loss: 0.1347 - val_mae: 0.2680 - lr: 0.0010 - _timestamp: 1648949496.0000 - _runtime: 1206.0000\n",
      "Epoch 8/30\n",
      "2485/2485 [==============================] - 152s 61ms/step - loss: 0.1517 - mae: 0.2881 - val_loss: 0.1230 - val_mae: 0.2550 - lr: 1.0000e-04 - _timestamp: 1648949648.0000 - _runtime: 1358.0000\n",
      "Epoch 9/30\n",
      "2485/2485 [==============================] - 153s 61ms/step - loss: 0.1459 - mae: 0.2827 - val_loss: 0.1200 - val_mae: 0.2516 - lr: 1.0000e-04 - _timestamp: 1648949800.0000 - _runtime: 1510.0000\n",
      "Epoch 10/30\n",
      "2485/2485 [==============================] - 150s 60ms/step - loss: 0.1443 - mae: 0.2811 - val_loss: 0.1185 - val_mae: 0.2502 - lr: 1.0000e-04 - _timestamp: 1648949950.0000 - _runtime: 1660.0000\n",
      "Epoch 11/30\n",
      "2485/2485 [==============================] - 150s 60ms/step - loss: 0.1425 - mae: 0.2791 - val_loss: 0.1169 - val_mae: 0.2485 - lr: 1.0000e-04 - _timestamp: 1648950100.0000 - _runtime: 1810.0000\n",
      "Epoch 12/30\n",
      "2485/2485 [==============================] - 149s 60ms/step - loss: 0.1416 - mae: 0.2784 - val_loss: 0.1152 - val_mae: 0.2463 - lr: 1.0000e-04 - _timestamp: 1648950249.0000 - _runtime: 1959.0000\n",
      "Epoch 13/30\n",
      "2485/2485 [==============================] - 149s 60ms/step - loss: 0.1401 - mae: 0.2766 - val_loss: 0.1143 - val_mae: 0.2450 - lr: 1.0000e-04 - _timestamp: 1648950397.0000 - _runtime: 2107.0000\n",
      "Epoch 14/30\n",
      "2485/2485 [==============================] - 155s 62ms/step - loss: 0.1400 - mae: 0.2763 - val_loss: 0.1139 - val_mae: 0.2445 - lr: 1.0000e-04 - _timestamp: 1648950552.0000 - _runtime: 2262.0000\n",
      "Epoch 15/30\n",
      "2485/2485 [==============================] - 178s 71ms/step - loss: 0.1391 - mae: 0.2755 - val_loss: 0.1190 - val_mae: 0.2494 - lr: 1.0000e-04 - _timestamp: 1648950730.0000 - _runtime: 2440.0000\n",
      "Epoch 16/30\n",
      "2485/2485 [==============================] - 193s 78ms/step - loss: 0.1384 - mae: 0.2748 - val_loss: 0.1128 - val_mae: 0.2432 - lr: 1.0000e-04 - _timestamp: 1648950922.0000 - _runtime: 2632.0000\n",
      "Epoch 17/30\n",
      "2485/2485 [==============================] - 222s 89ms/step - loss: 0.1377 - mae: 0.2739 - val_loss: 0.1116 - val_mae: 0.2418 - lr: 1.0000e-04 - _timestamp: 1648951145.0000 - _runtime: 2855.0000\n",
      "Epoch 18/30\n",
      "2485/2485 [==============================] - 207s 83ms/step - loss: 0.1380 - mae: 0.2745 - val_loss: 0.1132 - val_mae: 0.2440 - lr: 1.0000e-04 - _timestamp: 1648951352.0000 - _runtime: 3062.0000\n",
      "Epoch 19/30\n",
      "2485/2485 [==============================] - 194s 78ms/step - loss: 0.1397 - mae: 0.2764 - val_loss: 0.1130 - val_mae: 0.2440 - lr: 1.0000e-04 - _timestamp: 1648951545.0000 - _runtime: 3255.0000\n",
      "Epoch 20/30\n",
      "2485/2485 [==============================] - 180s 72ms/step - loss: 0.1386 - mae: 0.2751 - val_loss: 0.1123 - val_mae: 0.2431 - lr: 1.0000e-05 - _timestamp: 1648951725.0000 - _runtime: 3435.0000\n",
      "Epoch 21/30\n",
      "2485/2485 [==============================] - 187s 75ms/step - loss: 0.1382 - mae: 0.2746 - val_loss: 0.1125 - val_mae: 0.2433 - lr: 1.0000e-05 - _timestamp: 1648951912.0000 - _runtime: 3622.0000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.225 MB of 0.225 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>loss</td><td>█▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▂▂▂▂▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>mae</td><td>█▃▃▂▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▄▃▅▃▂▂▂▂▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>val_mae</td><td>█▅▄▄▃▅▃▂▂▂▂▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>16</td></tr><tr><td>best_val_loss</td><td>0.1116</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>loss</td><td>0.13819</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>mae</td><td>0.27462</td></tr><tr><td>val_loss</td><td>0.11247</td></tr><tr><td>val_mae</td><td>0.24331</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">zany-butterfly-14</strong>: <a href=\"https://wandb.ai/kmcguigan/time-series-methods/runs/o7660oed\" target=\"_blank\">https://wandb.ai/kmcguigan/time-series-methods/runs/o7660oed</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220402_191130-o7660oed\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\kiern\\MyFolders\\Code\\GitRepositories\\Time-Series-Playground\\wandb\\run-20220402_201212-20ailj3q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kmcguigan/time-series-methods/runs/20ailj3q\" target=\"_blank\">glad-puddle-15</a></strong> to <a href=\"https://wandb.ai/kmcguigan/time-series-methods\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3163/3163 [==============================] - 457s 120ms/step - loss: 0.3083 - mae: 0.4202 - val_loss: 0.2045 - val_mae: 0.3365 - lr: 0.0010 - _timestamp: 1648952408.0000 - _runtime: 476.0000\n",
      "Epoch 2/30\n",
      "3163/3163 [==============================] - 320s 101ms/step - loss: 0.1941 - mae: 0.3314 - val_loss: 0.1524 - val_mae: 0.2851 - lr: 0.0010 - _timestamp: 1648952729.0000 - _runtime: 797.0000\n",
      "Epoch 3/30\n",
      "3163/3163 [==============================] - 319s 101ms/step - loss: 0.1758 - mae: 0.3121 - val_loss: 0.1701 - val_mae: 0.2952 - lr: 0.0010 - _timestamp: 1648953047.0000 - _runtime: 1115.0000\n",
      "Epoch 4/30\n",
      "3163/3163 [==============================] - 326s 103ms/step - loss: 0.1587 - mae: 0.2959 - val_loss: 0.1437 - val_mae: 0.2724 - lr: 0.0010 - _timestamp: 1648953373.0000 - _runtime: 1441.0000\n",
      "Epoch 5/30\n",
      "3163/3163 [==============================] - 333s 105ms/step - loss: 0.1553 - mae: 0.2921 - val_loss: 0.1348 - val_mae: 0.2631 - lr: 0.0010 - _timestamp: 1648953706.0000 - _runtime: 1774.0000\n",
      "Epoch 6/30\n",
      "3163/3163 [==============================] - 546s 173ms/step - loss: 0.1482 - mae: 0.2850 - val_loss: 0.1310 - val_mae: 0.2576 - lr: 0.0010 - _timestamp: 1648954252.0000 - _runtime: 2320.0000\n",
      "Epoch 7/30\n",
      "3163/3163 [==============================] - 391s 124ms/step - loss: 0.1428 - mae: 0.2786 - val_loss: 0.1260 - val_mae: 0.2529 - lr: 0.0010 - _timestamp: 1648954643.0000 - _runtime: 2711.0000\n",
      "Epoch 8/30\n",
      "3163/3163 [==============================] - 400s 127ms/step - loss: 0.1551 - mae: 0.2898 - val_loss: 0.1267 - val_mae: 0.2518 - lr: 0.0010 - _timestamp: 1648955044.0000 - _runtime: 3112.0000\n",
      "Epoch 9/30\n",
      "3163/3163 [==============================] - 420s 133ms/step - loss: 0.1591 - mae: 0.2943 - val_loss: 0.1366 - val_mae: 0.2646 - lr: 0.0010 - _timestamp: 1648955464.0000 - _runtime: 3532.0000\n",
      "Epoch 10/30\n",
      "3163/3163 [==============================] - 429s 136ms/step - loss: 0.1432 - mae: 0.2792 - val_loss: 0.1305 - val_mae: 0.2562 - lr: 1.0000e-04 - _timestamp: 1648955893.0000 - _runtime: 3961.0000\n",
      "Epoch 11/30\n",
      "3163/3163 [==============================] - 437s 138ms/step - loss: 0.1398 - mae: 0.2754 - val_loss: 0.1239 - val_mae: 0.2498 - lr: 1.0000e-04 - _timestamp: 1648956330.0000 - _runtime: 4398.0000\n",
      "Epoch 12/30\n",
      "3163/3163 [==============================] - 471s 149ms/step - loss: 0.1368 - mae: 0.2723 - val_loss: 0.1221 - val_mae: 0.2470 - lr: 1.0000e-04 - _timestamp: 1648956801.0000 - _runtime: 4869.0000\n",
      "Epoch 13/30\n",
      "3163/3163 [==============================] - 466s 147ms/step - loss: 0.1347 - mae: 0.2699 - val_loss: 0.1243 - val_mae: 0.2497 - lr: 1.0000e-04 - _timestamp: 1648957267.0000 - _runtime: 5335.0000\n",
      "Epoch 14/30\n",
      "3163/3163 [==============================] - 485s 153ms/step - loss: 0.1339 - mae: 0.2690 - val_loss: 0.1197 - val_mae: 0.2440 - lr: 1.0000e-04 - _timestamp: 1648957752.0000 - _runtime: 5820.0000\n",
      "Epoch 15/30\n",
      "3163/3163 [==============================] - 428s 135ms/step - loss: 0.1327 - mae: 0.2676 - val_loss: 0.1219 - val_mae: 0.2471 - lr: 1.0000e-04 - _timestamp: 1648958180.0000 - _runtime: 6248.0000\n",
      "Epoch 16/30\n",
      "1379/3163 [============>.................] - ETA: 3:50 - loss: 0.1328 - mae: 0.2677"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
